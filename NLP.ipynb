{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38432bite89bfbd354db462cbaa1c98f000e7faa",
   "display_name": "Python 3.8.4 32-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NLP Methods\n",
    "\n",
    "In this notebook I will try more advanced NLP methods.\n",
    "I now have a baseline Naive Bayes of word counts at 67% to beat, which shouldnt be too hard.\n",
    "\n",
    "Will look at some text spam detection methods as the problem is similar enough to mine:\n",
    "- https://lionbridge.ai/articles/using-natural-language-processing-for-spam-detection-in-emails/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text  label  NumOfWords\n",
       "0      Entire Facebook Staff Laughs As Man Tightens P...      1           9\n",
       "1      Muslim Woman Denied Soda Can for Fear She Coul...      0          14\n",
       "2      Bold Move: Hulu Has Announced That They’re Gon...      1          23\n",
       "3      Despondent Jeff Bezos Realizes He’ll Have To W...      1          19\n",
       "4      For men looking for great single women, online...      1          23\n",
       "...                                                  ...    ...         ...\n",
       "23995  Teen Pregnancy Rate Prompting More High School...      1          14\n",
       "23996  Connecticut TV station under fire after using ...      0          15\n",
       "23997   Jurisprudence Fetishist Gets Off On Technicality      1           6\n",
       "23998  Employees From Other Department Announce Plan ...      1          17\n",
       "23999        Police drone crashes into police SWAT team       0           7\n",
       "\n",
       "[24000 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>NumOfWords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Entire Facebook Staff Laughs As Man Tightens P...</td>\n      <td>1</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Muslim Woman Denied Soda Can for Fear She Coul...</td>\n      <td>0</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Bold Move: Hulu Has Announced That They’re Gon...</td>\n      <td>1</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Despondent Jeff Bezos Realizes He’ll Have To W...</td>\n      <td>1</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>For men looking for great single women, online...</td>\n      <td>1</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23995</th>\n      <td>Teen Pregnancy Rate Prompting More High School...</td>\n      <td>1</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>23996</th>\n      <td>Connecticut TV station under fire after using ...</td>\n      <td>0</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>23997</th>\n      <td>Jurisprudence Fetishist Gets Off On Technicality</td>\n      <td>1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>23998</th>\n      <td>Employees From Other Department Announce Plan ...</td>\n      <td>1</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>23999</th>\n      <td>Police drone crashes into police SWAT team</td>\n      <td>0</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n<p>24000 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df = pd.read_csv('OnionOrNot.csv')\n",
    "df['NumOfWords'] = df.text.apply(lambda x: len(x.split()))\n",
    "df"
   ]
  },
  {
   "source": [
    "# Cleaning data\n",
    "I will now clean the data, which will involve:\n",
    "- Lowering case\n",
    "- Removal of special characters\n",
    "- Removal of stopwords\n",
    "- Removal of hyperlinks\n",
    "- Removal of numbers\n",
    "- Removal of whitespaces\n",
    "\n",
    "Will use the NLTK library for this and follow https://machinelearningmastery.com/clean-text-machine-learning-python/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Bold', 'Move', ':', 'Hulu', 'Has', 'Announced', 'That', 'They', '’', 're']\n['Bold', 'Move', 'Hulu', 'Has', 'Announced', 'That', 'They', 're', 'Gon', 'na']\n['Bold', 'Move', 'Hulu', 'Has', 'Announced', 'That', 'They', 'Gon', 'na', 'Go']\n"
     ]
    }
   ],
   "source": [
    "# TODO: turn this into a function so I can Lambda through all of the text in pandas\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# using the text at index 2 as an example here, just becasue it contains lots of non letter characters\n",
    "text = df.text[2]\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:10])    # show only first 10\n",
    "\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:10])\n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}